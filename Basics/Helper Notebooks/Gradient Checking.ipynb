{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc44026e-c0d7-4cef-9ea4-a925e2367901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "from nn_building_blocks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c110013f-3f3f-494b-b944-7257436cb2f8",
   "metadata": {},
   "source": [
    "### 1D Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516b2303-58b6-446e-8d6a-0673c4399176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta):\n",
    "    J = x * theta\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c200425c-980b-41b3-a390-c4fcb47b4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x, theta):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39235046-cf87-4298-9b20-c84d7a8fc9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x, theta, epsilon=1e-7, print_msg=False):\n",
    "    theta_plus = theta + epsilon\n",
    "    theta_minus = theta - epsilon\n",
    "    J_plus = forward_propagation(x, theta_plus)\n",
    "    J_minus = forward_propagation(x, theta_minus)\n",
    "    gradapprox = (J_plus-J_minus) / (2*epsilon)\n",
    "\n",
    "    grad = backward_propagation(x, theta)\n",
    "\n",
    "    numerator = np.linalg.norm(grad-gradapprox)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "\n",
    "    difference = numerator / denominator\n",
    "    \n",
    "    if print_msg:\n",
    "        if difference > 2e-7:\n",
    "            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "        else:\n",
    "            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f1b412-f849-4e09-9ab2-b390f611096e",
   "metadata": {},
   "source": [
    "### N-D Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac8531b-c5e4-444e-b83d-f13ff6f5e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    m = X.shape[1]\n",
    "    caches = []\n",
    "    L = len(parameters)//2\n",
    "    A = X\n",
    "\n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], \"relu\")\n",
    "        caches += [cache[1], A, cache[0][1], cache[0][2]]\n",
    "\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"sigmoid\")\n",
    "    caches += [cache[1], AL, cache[0][1], cache[0][2]]\n",
    "\n",
    "    cost = (1./m) * np.sum(np.multiply(-np.log(AL), Y) + np.multiply(-np.log(1-AL), 1-Y))\n",
    "    \n",
    "    caches = tuple(caches)\n",
    "    return cost, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6326bd-e940-4e9a-b3ce-8d8a4d5ac417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X, Y, cache):\n",
    "    grads = {}\n",
    "    L = len(cache)//4\n",
    "    m = X.shape[1]\n",
    "    AL = cache[-3]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "\n",
    "    dZL = AL - Y\n",
    "    dWL = (1 / m) * np.dot(dZL, cache[-7].T)\n",
    "    dbL = (1 / m) * np.sum(dZL, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(cache[-2].T, dZL)\n",
    "\n",
    "    # grads[\"dA\" + str(L-1)] = dA_prev\n",
    "    # grads[\"dZ\" + str(L)] = dZL\n",
    "    grads[\"dW\" + str(L)] = dWL\n",
    "    grads[\"db\" + str(L)] = dbL\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        dA_current = dA_prev\n",
    "        \n",
    "        Zl = cache[((l+1)*4)-4]\n",
    "        Al = cache[((l+1)*4)-3]\n",
    "        Wl = cache[((l+1)*4)-2]\n",
    "        bl = cache[((l+1)*4)-1]\n",
    "        \n",
    "        if l > 0:\n",
    "            A_prev = cache[l*4-3]\n",
    "        else:\n",
    "            A_prev = X\n",
    "        \n",
    "        dZl = np.multiply(dA_current, np.int64(Al > 0))\n",
    "        \n",
    "        dA_prev, dWl, dbl = linear_activation_backward(dA_current, ((A_prev, Wl, bl), Zl) ,\"relu\")\n",
    "        \n",
    "        # grads[\"dA\" + str(l)] = dA_prev\n",
    "        # grads[\"dZ\" + str(l+1)] = dZl\n",
    "        grads[\"dW\" + str(l+1)] = dWl\n",
    "        grads[\"db\" + str(l+1)] = dbl    \n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f1f13b0-b4b0-48fc-ada8-474eb87b0d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_to_vector(parameters):\n",
    "\n",
    "    keys = []\n",
    "    theta = []\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        \n",
    "        for param in ['W', 'b']:\n",
    "            key = param + str(l)\n",
    "            vector = parameters[key].reshape(-1, 1)\n",
    "            keys += [key] * vector.shape[0]\n",
    "            theta.append(vector)\n",
    "    \n",
    "    theta = np.concatenate(theta, axis=0)\n",
    "    return theta, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e679aea4-91e7-45c1-8ebf-8863ba64c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients_to_vector(gradients):\n",
    "    \n",
    "    theta = []\n",
    "    L = len(gradients) // 2\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        \n",
    "        for grad in ['dW', 'db']:\n",
    "            key = grad + str(l)\n",
    "            vector = gradients[key].reshape(-1, 1)\n",
    "            theta.append(vector)\n",
    "\n",
    "    theta = np.concatenate(theta, axis=0)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2c26be-4038-4530-a46d-3e2bbe2ee063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_dictionary(theta, shape_reference):\n",
    "    parameters = {}\n",
    "    cursor = 0\n",
    "    for l in range(1, len(shape_reference) //2 + 1):\n",
    "        Wl_shape = shape_reference[\"W\"+str(l)].shape\n",
    "        bl_shape = shape_reference[\"b\"+str(l)].shape\n",
    "\n",
    "        size_W = Wl_shape[0]*Wl_shape[1]\n",
    "        parameters[\"W\"+str(l)] = theta[cursor: cursor+size_W].reshape(Wl_shape)\n",
    "        cursor += size_W\n",
    "\n",
    "        size_b = bl_shape[0]*bl_shape[1]\n",
    "        parameters[\"b\"+str(l)] = theta[cursor: cursor+size_b].reshape(bl_shape)\n",
    "        cursor += size_b\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f88c0890-53a0-4202-9cd2-ae708fa532bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7, print_msg=False):\n",
    "    parameters_values, keys = dictionary_to_vector(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "\n",
    "    \n",
    "    num_parameters = len(parameters_values)\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "\n",
    "    for i in range(num_parameters):\n",
    "        theta_plus = np.copy(parameters_values)\n",
    "        theta_plus[i] += epsilon\n",
    "        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_plus,parameters))\n",
    "\n",
    "        theta_minus = np.copy(parameters_values)\n",
    "        theta_minus[i] -=  epsilon\n",
    "        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_minus,parameters))\n",
    "\n",
    "        gradapprox[i] = (J_plus[i]-J_minus[i]) / (2*epsilon)\n",
    "\n",
    "    numerator = np.linalg.norm(grad - gradapprox)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "    difference = (numerator) / (denominator)\n",
    "    \n",
    "    if print_msg:\n",
    "        if difference > 2e-7:\n",
    "            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "        else:\n",
    "            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "            \n",
    "    return difference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (ML)",
   "language": "python",
   "name": "ml3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
